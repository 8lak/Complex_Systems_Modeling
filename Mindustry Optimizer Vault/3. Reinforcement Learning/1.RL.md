Reinforcement learning is the idea of mapping situations to actions such that the actions maximize the reward. The learner (agent) is not told what actions to take but learns them by following the reward system. The learner must learn through actions and said actions can affect immediate and subsequent rewards. The two features are trial-and-error search and delayed rewards. The two most important distinguishing features of RL. 

Main idea: capture the essence of the real world problem such that the agent can accurately reflect plausible actions. The agent must be able to sense the state of the environment to some extent since that influences the current action, it must also have goals to understand what actions to prioritize, and definable actions inside each state. Sensation, actions, goal.

Key dilemma exploration vs exploitation. Obtaining rewards via known circuits that are reliable is considered exploitation exploring potentially more rewarding actions that are unknown is considered exploration. The best course is a balance of both where the agent tries actions repeatedly and "learns" which actions on average end up with more rewards than others. See book chapter 2 for more in depth exposure to dilemma.

Key feature is that RL is a top down approach where it considers the whole problem and eventual generalization from it 

Beyond the agent and environment we have policy, reward function, value function and model of environment.

- **Policy**: The core of the RL model it defines the set of actions that the agent can take in a particular state.
- **Reward Function**: Defines the goal. maps the state-action pairs or sub elements of the policy to a single value which is considered the reward. Tend to be stochastic (random probability distribution) considered immediate
- **Value Function** : Same idea as reward function but for long-term. Value of a state is the total reward an agent can expect to receive starting from some state. 
- **Model of environment**: 



Side note on value functions: Most RL models are based around this question of developing better value function but if the policy space is small enough and the good policies are easy to find the book suggest that the usage of search methods such as genetic algorithms, genetic programming, simulated annealing can be as reliable if not more so when the sensation or state of the environment cannot be aptly perceived. Also known as evolutionary methods. 


The biggest difference is in feedback between RL and SL is evaluative vs instructive. Evaluative indicates how good an independent action was instructive just directs which action is the best. 

**N-armed bandit**

In this scenario we use a *non-associative* version which implies that actions are taken in only one scenario 